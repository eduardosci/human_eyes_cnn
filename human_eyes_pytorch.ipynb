{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0f09b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduardo/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/eduardo/anaconda3/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104impl8GPUTrace13gpuTraceStateE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "import torch , torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import time\n",
    "from IPython.display import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm  # Importar a biblioteca tqdm para a barra de progresso\n",
    "import shutil\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f700f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '/home/eduardo/projetos/human_eyes/data'\n",
    "\n",
    "path_train = os.path.join(dataset, 'train')\n",
    "path_test = os.path.join(dataset, 'test')\n",
    "path_valid = os.path.join(dataset, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e767357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_test = os.path.join(dataset, 'test')\n",
    "# os.makedirs(path_test, exist_ok=True)\n",
    "\n",
    "# # Lista de subpastas dentro da pasta de treinamento\n",
    "# subfolders = ['close eyes', 'open eyes']\n",
    "\n",
    "# # Para cada subpasta, mova 20% das imagens para a pasta de teste\n",
    "# for subfolder in subfolders:\n",
    "#     subfolder_path_train = os.path.join(path_train, subfolder)\n",
    "#     subfolder_path_test = os.path.join(path_test, subfolder)\n",
    "#     os.makedirs(subfolder_path_test, exist_ok=True)\n",
    "\n",
    "#     files = os.listdir(subfolder_path_train)\n",
    "#     num_files = len(files)\n",
    "#     num_files_to_move = int(0.1 * num_files)\n",
    "\n",
    "#     # Embaralhe os arquivos para evitar vieses\n",
    "#     random.shuffle(files)\n",
    "\n",
    "#     files_to_move = files[:num_files_to_move]\n",
    "\n",
    "#     # Movendo os arquivos para a pasta de teste\n",
    "#     for file in files_to_move:\n",
    "#         src = os.path.join(subfolder_path_train, file)\n",
    "#         dst = os.path.join(subfolder_path_test, file)\n",
    "#         shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db998fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs = torch.stack([img for img, _ in data_loader_train], dim=3)\n",
    "# imgs.view(3, -1).mean(dim= 1), imgs.view(3, -1).std(dim= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6c6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_mean_and_std(data_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f854fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 180\n",
    "\n",
    "mean = [0.3506, 0.3506, 0.3506]\n",
    "std = [0.0187, 0.0187, 0.0187]\n",
    "\n",
    "images_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(size=[image_size, image_size]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.RandomPerspective(),\n",
    "        #transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.5),  # Adiciona desfoque aleatório\n",
    "        #transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.2),  # Converte para escala de cinza aleatoriamente\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.1), ratio=(0.2, 3.3))  # Adiciona Random Erasing\n",
    "        \n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=[image_size, image_size]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "\n",
    "    ]),\n",
    "      'valid': transforms.Compose([\n",
    "        transforms.Resize(size=[image_size, image_size]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "\n",
    "\n",
    "    ]),      \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637d9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "\n",
    "class_numbers = len(os.listdir(path_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a4b1e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "884563ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'train': datasets.ImageFolder(root= path_train, transform=images_transform ['train']),\n",
    "    'test': datasets.ImageFolder(root= path_valid, transform=images_transform ['test'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d20366a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'close eyes', 1: 'open eyes'}\n"
     ]
    }
   ],
   "source": [
    "idx_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
    "print(idx_class)\n",
    "\n",
    "imgs_train = len(data['train'])\n",
    "imgs_test = len(data['test'])\n",
    "\n",
    "data_loader_train = DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
    "data_loader_test = DataLoader(data['test'], batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446d612b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for classe in idx_class.values():\n",
    "    path_classe = os.path.join(path_test, classe)\n",
    "    arq_img = os.listdir(path_classe)[1]\n",
    "    img = Image(filename = os.path.join(path_classe, arq_img))\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94cb8c92",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduardo/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eduardo/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 1/40 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 129>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    128\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m\n\u001b[0;32m--> 129\u001b[0m model_trained, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43malexnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, metric_error, optimizer, scheduler, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Training loop with tqdm progress bar\u001b[39;00m\n\u001b[1;32m     19\u001b[0m training_bar \u001b[38;5;241m=\u001b[39m tqdm(data_loader_train, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data_loader_train), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (enters, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_bar):\n\u001b[1;32m     21\u001b[0m     enters \u001b[38;5;241m=\u001b[39m enters\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/datasets/folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:1283\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_brightness(img, brightness_factor)\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m contrast_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1283\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m saturation_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1285\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py:915\u001b[0m, in \u001b[0;36madjust_contrast\u001b[0;34m(img, contrast_factor)\u001b[0m\n\u001b[1;32m    913\u001b[0m     _log_api_usage_once(adjust_contrast)\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39madjust_contrast(img, contrast_factor)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:82\u001b[0m, in \u001b[0;36madjust_contrast\u001b[0;34m(img, contrast_factor)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pil_image(img):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m enhancer \u001b[38;5;241m=\u001b[39m \u001b[43mImageEnhance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mContrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m img \u001b[38;5;241m=\u001b[39m enhancer\u001b[38;5;241m.\u001b[39menhance(contrast_factor)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/PIL/ImageEnhance.py:67\u001b[0m, in \u001b[0;36mContrast.__init__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage \u001b[38;5;241m=\u001b[39m image\n\u001b[0;32m---> 67\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(ImageStat\u001b[38;5;241m.\u001b[39mStat(\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mmean[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegenerate \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m, image\u001b[38;5;241m.\u001b[39msize, mean)\u001b[38;5;241m.\u001b[39mconvert(image\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m image\u001b[38;5;241m.\u001b[39mgetbands():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/PIL/Image.py:1016\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     dither \u001b[38;5;241m=\u001b[39m FLOYDSTEINBERG\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1016\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdither\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;66;03m# normalize source image and try again\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, metric_error, optimizer, scheduler, epochs):\n",
    "    history = []\n",
    "    best_accuracy = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        print('\\n\\nEpoch: {}/{}'.format(epoch + 1, epochs), end=\" \")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        error_train = 0.0\n",
    "        acc_train = 0.0\n",
    "\n",
    "        error_test = 0.0\n",
    "        acc_test = 0.0\n",
    "        \n",
    "        # Training loop with tqdm progress bar\n",
    "        training_bar = tqdm(data_loader_train, total=len(data_loader_train), desc=\"Training\", leave=False)\n",
    "        for i, (enters, labels) in enumerate(training_bar):\n",
    "            enters = enters.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(enters)\n",
    "            error = metric_error(outputs, labels)\n",
    "            error.backward()\n",
    "            optimizer.step()\n",
    "            error_train += error.item() * enters.size(0)\n",
    "\n",
    "            max_values, idx_max_values = torch.max(outputs.data, 1)\n",
    "            true_pred = idx_max_values.eq(labels.data.view_as(idx_max_values))\n",
    "\n",
    "            accuracy = torch.mean(true_pred.type(torch.FloatTensor))\n",
    "            acc_train += accuracy.item() * enters.size(0)\n",
    "\n",
    "            # Atualizar descrição da barra de progresso\n",
    "            training_bar.set_postfix({\"Error\": error.item(), \"Accuracy\": accuracy.item()})\n",
    "\n",
    "        # Testing loop with tqdm progress bar\n",
    "        testing_bar = tqdm(data_loader_test, total=len(data_loader_test), desc=\"Testing\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for j, (enters, labels) in enumerate(testing_bar):\n",
    "                enters = enters.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(enters)\n",
    "                error = metric_error(outputs, labels)\n",
    "                error_test += error.item() * enters.size(0)\n",
    "\n",
    "                max_values, idx_max_values = torch.max(outputs.data, 1)\n",
    "                true_pred = idx_max_values.eq(labels.data.view_as(idx_max_values))\n",
    "\n",
    "                accuracy = torch.mean(true_pred.type(torch.FloatTensor))\n",
    "                acc_test += accuracy.item() * enters.size(0)\n",
    "\n",
    "                # Atualizar descrição da barra de progresso\n",
    "                testing_bar.set_postfix({\"Error\": error.item(), \"Accuracy\": accuracy.item()})\n",
    "\n",
    "        mean_error_train = error_train / imgs_train\n",
    "        mean_acc_train = acc_train / imgs_train\n",
    "\n",
    "        mean_error_test = error_test / imgs_test\n",
    "        mean_acc_test = acc_test / imgs_test\n",
    "\n",
    "        history.append([mean_error_train, mean_error_test, mean_acc_train, mean_acc_test])\n",
    "        end_epoch = time.time()\n",
    "\n",
    "        print('Train: Error: {:.4f}, acc {:.4f}%, Test: Error: {:.4f}, acc {:.4f}%'.format(mean_error_train, mean_acc_train * 100, mean_error_test, mean_acc_test * 100), end=\" | \")\n",
    "        \n",
    "        # Atualizar o scheduler no final da época com a métrica de erro do treinamento\n",
    "        scheduler.step(mean_error_train)\n",
    "        \n",
    "        if mean_acc_test > best_accuracy:\n",
    "            best_accuracy = mean_acc_test\n",
    "            model_save_path = '/home/eduardo/projetos/human_eyes/best_model.pt'\n",
    "            torch.save(model, model_save_path)\n",
    "            best_model = model\n",
    "            \n",
    "            # Salvar o estado do modelo e do otimizador\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'history': history,\n",
    "                'best_accuracy': best_accuracy,\n",
    "            }\n",
    "            torch.save(checkpoint, '/home/eduardo/projetos/human_eyes/checkpoint.pth')\n",
    "\n",
    "    return best_model, history\n",
    "\n",
    "# Definir função para inicialização de pesos\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "# Carregar modelo AlexNet pré-treinado\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "# Aplicar a inicialização de pesos personalizada\n",
    "alexnet.apply(init_weights)\n",
    "\n",
    "# Carregar modelo AlexNet pré-treinado\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modificar o classificador para o número de classes\n",
    "class_numbers = 2  # Substitua pelo número correto de classes\n",
    "alexnet.classifier[6] = nn.Linear(4096, class_numbers)\n",
    "# Adicionar Batch Normalization após as camadas Conv2d\n",
    "alexnet.features.add_module('7_bn', nn.BatchNorm2d(256))  # Ajuste o número de features conforme necessário\n",
    "alexnet.features.add_module('8_relu', nn.ReLU(inplace=True))  # Adicione a ativação ReLU\n",
    "alexnet.classifier.add_module('7', nn.Dropout(0.5))\n",
    "alexnet.classifier.add_module('8', nn.LogSigmoid())\n",
    "alexnet.classifier[8] = nn.Softmax(dim=1)\n",
    "\n",
    "# Adicionar a penalização L2 aos parâmetros do otimizador\n",
    "optimizer = torch.optim.Adam(alexnet.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Definir o scheduler ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1, verbose=True)\n",
    "\n",
    "# Chamar a função de treinamento\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 40\n",
    "model_trained, history = train_model(alexnet, loss, optimizer, scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extrair informações do histórico\n",
    "# train_losses = [item[0] for item in history]\n",
    "# test_losses = [item[1] for item in history]\n",
    "# train_accuracies = [item[2] for item in history]\n",
    "# test_accuracies = [item[3] for item in history]\n",
    "\n",
    "# # Plotar resultados\n",
    "# plt.figure(figsize=(12, 4))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(range(1, num_epochs + 1), train_losses, label='Treinamento')\n",
    "# plt.plot(range(1, num_epochs + 1), test_losses, label='Teste')\n",
    "# plt.xlabel('Época')\n",
    "# plt.ylabel('Perda')\n",
    "# plt.title('Curva de Perda')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(range(1, num_epochs + 1), train_accuracies, label='Treinamento')\n",
    "# plt.plot(range(1, num_epochs + 1), test_accuracies, label='Teste')\n",
    "# plt.xlabel('Época')\n",
    "# plt.ylabel('Acurácia')\n",
    "# plt.title('Curva de Acurácia')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ca117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
